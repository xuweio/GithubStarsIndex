#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GitHub Stars IndexåŒæ­¥è„šæœ¬ (JSON + Template ç‰ˆ)
åŠŸèƒ½ï¼š
  1. ä» GitHub API æŠ“å–ç”¨æˆ· Star çš„é¡¹ç›®åˆ—è¡¨
  2. å¢é‡è·å– README å¹¶è°ƒç”¨ AI ç”Ÿæˆæ‘˜è¦ï¼Œå­˜å‚¨è‡³ JSON æ•°æ®é›†
  3. ä½¿ç”¨ Jinja2 æ¨¡æ¿å°† JSON æ•°æ®æ¸²æŸ“ä¸º Markdown
  4. æ”¯æŒæ¨é€åˆ° Obsidian Vault ä»“åº“
"""

import os
import sys
import json
import time
import base64
import logging
import threading
from datetime import datetime, timezone
from pathlib import Path
from typing import Optional
from concurrent.futures import ThreadPoolExecutor

import requests
import yaml
from openai import OpenAI
from jinja2 import Environment, FileSystemLoader

# â”€â”€ æ—¥å¿—é…ç½® â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)
log = logging.getLogger(__name__)

# â”€â”€ å¸¸é‡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SCRIPT_DIR = Path(__file__).parent.parent  # ä»“åº“æ ¹ç›®å½•
CONFIG_PATH = SCRIPT_DIR / "config.yml"
DATA_DIR = SCRIPT_DIR / "data"
STARS_JSON_PATH = DATA_DIR / "stars.json"
TEMPLATES_DIR = SCRIPT_DIR / "templates"
DEFAULT_MD_TEMPLATE = "stars.md.j2"
STARS_MD_PATH_DEFAULT = SCRIPT_DIR / "stars.md"

# ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨
DATA_DIR.mkdir(exist_ok=True)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# é…ç½®åŠ è½½
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


def load_config() -> dict:
    """åŠ è½½ config.ymlï¼Œå¹¶ç”¨ç¯å¢ƒå˜é‡è¦†ç›–æ•æ„Ÿå­—æ®µ"""
    if not CONFIG_PATH.exists():
        log.error(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {CONFIG_PATH}")
        sys.exit(1)

    with open(CONFIG_PATH, "r", encoding="utf-8") as f:
        cfg = yaml.safe_load(f) or {}

    # ç¯å¢ƒå˜é‡ä¼˜å…ˆè¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„å€¼
    if os.environ.get("GH_USERNAME"):
        cfg["github"]["username"] = os.environ["GH_USERNAME"]

    if os.environ.get("AI_BASE_URL"):
        cfg["ai"]["base_url"] = os.environ["AI_BASE_URL"]
    if os.environ.get("AI_API_KEY"):
        cfg["ai"]["api_key"] = os.environ["AI_API_KEY"]
    if os.environ.get("AI_MODEL"):
        cfg["ai"]["model"] = os.environ["AI_MODEL"]

    if os.environ.get("GH_TOKEN") or os.environ.get("GITHUB_TOKEN"):
        cfg["github"]["token"] = os.environ.get("GH_TOKEN") or os.environ.get(
            "GITHUB_TOKEN"
        )
    else:
        cfg["github"]["token"] = None

    vault = cfg.get("vault_sync", {})
    if os.environ.get("VAULT_SYNC_ENABLED", "").lower() == "true":
        vault["enabled"] = True
    if os.environ.get("VAULT_REPO"):
        vault["repo"] = os.environ["VAULT_REPO"]
    if os.environ.get("VAULT_FILE_PATH"):
        vault["file_path"] = os.environ["VAULT_FILE_PATH"]
    if os.environ.get("VAULT_PAT"):
        vault["pat"] = os.environ["VAULT_PAT"]
    cfg["vault_sync"] = vault

    # æµ‹è¯•é™åˆ¶ï¼ˆå¯é€‰ï¼‰
    test_limit = os.environ.get("TEST_LIMIT", "")
    cfg["test_limit"] = int(test_limit) if test_limit.isdigit() else None

    # å¹¶å‘æ§åˆ¶
    concurrency = os.environ.get("MAX_CONCURRENCY", "")
    if concurrency.isdigit():
        cfg["ai"]["concurrency"] = int(concurrency)
    elif "concurrency" not in cfg["ai"]:
        cfg["ai"]["concurrency"] = 5

    return cfg


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æ•°æ®å­˜å‚¨
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class DataStore:
    def __init__(self, path: Path):
        self.path = path
        self.lock = threading.Lock()
        self.data = self._load()

    def _load(self) -> dict:
        if not self.path.exists():
            return {"last_updated": "", "repos": {}}
        try:
            with open(self.path, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            log.error(f"åŠ è½½æ•°æ®æ–‡ä»¶å¤±è´¥: {e}")
            return {"last_updated": "", "repos": {}}

    def save(self):
        with self.lock:
            self.data["last_updated"] = datetime.now(timezone.utc).strftime(
                "%Y-%m-%d %H:%M UTC"
            )
            with open(self.path, "w", encoding="utf-8") as f:
                json.dump(self.data, f, ensure_ascii=False, indent=2)

    def update_repo(self, full_name: str, metadata: dict, summary: dict):
        with self.lock:
            self.data["repos"][full_name] = {
                "metadata": metadata,
                "summary": summary,
                "pushed_at": metadata.get("updated_at", ""),
                "updated_at": datetime.now(timezone.utc).strftime("%Y-%m-%d"),
            }

    def get_repo(self, full_name: str) -> Optional[dict]:
        return self.data["repos"].get(full_name)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# GitHub API å®¢æˆ·ç«¯
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class GitHubClient:
    BASE_URL = "https://api.github.com"

    def __init__(self, username: str, token: Optional[str] = None):
        self.username = username
        self.session = requests.Session()
        self.session.headers.update(
            {
                "Accept": "application/vnd.github+json",
                "X-GitHub-Api-Version": "2022-11-28",
            }
        )
        if token:
            self.session.headers["Authorization"] = f"Bearer {token}"

    def _get(self, url: str, params: dict = None) -> requests.Response:
        for attempt in range(3):
            try:
                resp = self.session.get(url, params=params, timeout=30)
                if resp.status_code == 403 and "rate limit" in resp.text.lower():
                    reset_time = int(
                        resp.headers.get("X-RateLimit-Reset", time.time() + 60)
                    )
                    wait = max(reset_time - int(time.time()), 5)
                    log.warning(f"API é™é€Ÿï¼Œç­‰å¾… {wait} ç§’...")
                    time.sleep(wait)
                    continue
                resp.raise_for_status()
                return resp
            except requests.RequestException as e:
                log.warning(f"è¯·æ±‚å¤±è´¥ï¼ˆç¬¬ {attempt + 1} æ¬¡ï¼‰: {e}")
                time.sleep(2**attempt)
        raise Exception("å¤šæ¬¡è¯·æ±‚å¤±è´¥")

    def get_starred_repos(self) -> list[dict]:
        repos = []
        page = 1
        log.info(f"æ­£åœ¨æŠ“å– @{self.username} çš„ Stars...")
        while True:
            url = f"{self.BASE_URL}/users/{self.username}/starred"
            resp = self._get(
                url,
                params={
                    "per_page": 100,
                    "page": page,
                    "sort": "created",
                    "direction": "desc",
                },
            )
            data = resp.json()
            if not data:
                break
            for item in data:
                repos.append(
                    {
                        "full_name": item["full_name"],
                        "name": item["name"],
                        "owner": item["owner"]["login"],
                        "description": item.get("description") or "",
                        "stars": item["stargazers_count"],
                        "language": item.get("language") or "N/A",
                        "url": item["html_url"],
                        "homepage": item.get("homepage") or "",
                        "topics": item.get("topics", []),
                        "updated_at": item.get("pushed_at", "")[:10],
                    }
                )
            log.info(f"  ç¬¬ {page} é¡µï¼šè·å– {len(data)} ä¸ªï¼Œå…± {len(repos)} ä¸ª")
            if "next" not in resp.headers.get("Link", ""):
                break
            page += 1
        return repos

    def get_readme(self, full_name: str, max_length: int) -> str:
        url = f"{self.BASE_URL}/repos/{full_name}/readme"
        try:
            resp = self._get(url)
            data = resp.json()
            content = base64.b64decode(data["content"]).decode("utf-8", errors="ignore")
            return content[:max_length]
        except Exception:
            return ""

    def push_file(self, repo: str, path: str, content: str, msg: str, pat: str) -> bool:
        url = f"{self.BASE_URL}/repos/{repo}/contents/{path}"
        headers = {
            "Authorization": f"Bearer {pat}",
            "Accept": "application/vnd.github+json",
        }
        sha = None
        try:
            r = requests.get(url, headers=headers, timeout=30)
            if r.status_code == 200:
                sha = r.json().get("sha")
        except Exception:
            pass
        payload = {
            "message": msg,
            "content": base64.b64encode(content.encode("utf-8")).decode("ascii"),
        }
        if sha:
            payload["sha"] = sha
        try:
            r = requests.put(url, headers=headers, json=payload, timeout=30)
            r.raise_for_status()
            log.info(f"âœ… å·²æ¨é€è‡³: {repo}/{path}")
            return True
        except Exception as e:
            log.error(f"âŒ æ¨é€å¤±è´¥: {e}")
            return False


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# AI æ‘˜è¦ç”Ÿæˆ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class AISummarizer:
    def __init__(
        self, base_url: str, api_key: str, model: str, timeout: int = 60, retry: int = 3
    ):
        self.model = model
        self.retry = retry
        self.client = OpenAI(base_url=base_url, api_key=api_key, timeout=timeout)

    def summarize(self, repo_name: str, description: str, readme: str) -> dict:
        context = f"Repo: {repo_name}\nDesc: {description}\n\nREADME:\n{readme}"
        prompt = """ä½ æ˜¯ä¸€ä¸ªæŠ€æœ¯æ–‡æ¡£åˆ†æä¸“å®¶ã€‚è¯·æ ¹æ® GitHub ä»“åº“ä¿¡æ¯ç”Ÿæˆï¼š
1. ä¸“ä¸šçš„**ä¸­æ–‡æ‘˜è¦**ï¼ˆ100å­—ä»¥å†…ï¼‰ï¼Œæè¿°æ ¸å¿ƒåŠŸèƒ½ã€åœºæ™¯å’Œäº®ç‚¹
2. **å…³é”®è¯æ ‡ç­¾**ï¼ˆ5-8ä¸ªï¼‰

è¾“å‡º JSON æ ¼å¼ï¼š
{
  "zh": "æ‘˜è¦å†…å®¹",
  "tags": ["tag1", "tag2"]
}"""
        for attempt in range(self.retry):
            try:
                resp = self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": prompt},
                        {"role": "user", "content": context},
                    ],
                    temperature=0.3,
                    response_format={"type": "json_object"},
                )
                return json.loads(resp.choices[0].message.content)
            except Exception as e:
                if attempt == self.retry - 1:
                    log.error(f"AI ç”Ÿæˆå¤±è´¥ [{repo_name}]: {e}")
                    return {"zh": "ç”Ÿæˆå¤±è´¥", "tags": []}
                log.warning(f"AI é‡è¯• {attempt + 1}...")
                time.sleep(2**attempt)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æ¨¡ç‰ˆç”Ÿæˆå™¨
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class TemplateGenerator:
    def __init__(self, template_dir: Path):
        self.env = Environment(
            loader=FileSystemLoader(str(template_dir)),
            trim_blocks=True,
            lstrip_blocks=True,
        )

    def render(self, template_name: str, context: dict) -> str:
        template = self.env.get_template(template_name)
        return template.render(context)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ä¸»æµç¨‹
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


def main():
    log.info("GitHub Stars IndexåŒæ­¥ç³»ç»Ÿå¼€å§‹è¿è¡Œ")
    cfg = load_config()

    gh = GitHubClient(cfg["github"]["username"], cfg["github"].get("token"))
    ai = AISummarizer(
        cfg["ai"]["base_url"],
        cfg["ai"]["api_key"],
        cfg["ai"]["model"],
        cfg["ai"].get("timeout", 60),
        cfg["ai"].get("max_retries", 3),
    )
    store = DataStore(STARS_JSON_PATH)
    generator = TemplateGenerator(TEMPLATES_DIR)

    # 1. æŠ“å–æ‰€æœ‰ Stars
    all_repos = gh.get_starred_repos()

    # 2. å¢é‡å¤„ç†
    new_repos_to_process = []
    seen_full_names = set()  # é˜²æ­¢ API è¿”å›é‡å¤æ•°æ®
    test_limit = cfg.get("test_limit")

    for repo in all_repos:
        full_name = repo["full_name"]

        # è·³è¿‡å·²ç»åœ¨æ­¤æ¬¡è¿è¡Œä¸­å¤„ç†è¿‡æˆ–å·²å­˜åœ¨äº JSON ä¸­çš„
        if full_name in seen_full_names:
            continue

        existing = store.get_repo(full_name)
        if not existing:
            if test_limit is not None and len(new_repos_to_process) >= test_limit:
                continue
            new_repos_to_process.append(repo)
            seen_full_names.add(full_name)
        else:
            # æ›´æ–°å…ƒæ•°æ®ä¿¡æ¯ï¼ˆStars æ•°ç­‰ï¼‰ä½†ä¿ç•™æ—§æ‘˜è¦
            existing["metadata"] = repo
            seen_full_names.add(full_name)

    def process_repo(args):
        idx, repo_data = args
        fname = repo_data["full_name"]
        total = len(new_repos_to_process)

        log.info(f"[{idx}/{total}] æ­£åœ¨å¤„ç†æ–°ä»“åº“: {fname}")
        readme_content = gh.get_readme(fname, cfg["ai"].get("max_readme_length", 4000))

        if not readme_content and not repo_data["description"]:
            summ = {"zh": "æš‚æ— æè¿°ã€‚", "tags": []}
        else:
            summ = ai.summarize(fname, repo_data["description"], readme_content)

        store.update_repo(fname, repo_data, summ)
        return True

    new_count = len(new_repos_to_process)
    if new_count > 0:
        concurrency = cfg["ai"].get("concurrency", 5)
        log.info(f"ğŸš€ å¼€å§‹å¹¶å‘å¤„ç† {new_count} ä¸ªæ–°ä»“åº“ (å¹¶å‘æ•°: {concurrency})")
        with ThreadPoolExecutor(max_workers=concurrency) as executor:
            list(executor.map(process_repo, enumerate(new_repos_to_process, 1)))

    if new_count > 0:
        store.save()
        log.info(f"âœ… æ•°æ®ä¿å­˜å®Œæˆï¼Œæ–°å¢ {new_count} æ¡è®°å½•")
    else:
        log.info("âœ¨ æ²¡æœ‰æ–°æ¡ç›®éœ€è¦å¤„ç†")

    # 3. æŒ‰ Star æ—¶é—´é‡æ–°æ’åºï¼ˆæœ€æ–° Star åœ¨å‰ï¼‰
    # JSON é‡Œçš„ repos æ˜¯æ— åºçš„ï¼Œæˆ‘ä»¬æŒ‰ç…§ all_repos çš„é¡ºåºæ¥ç”Ÿæˆï¼ˆå®ƒæ˜¯å€’åºçš„ï¼‰
    ordered_repos = []
    for r_meta in all_repos:
        entry = store.get_repo(r_meta["full_name"])
        if entry:
            # åˆå¹¶å±•ç¤ºéœ€è¦çš„æ•°æ®
            view_data = {**entry["metadata"], "summary": entry["summary"]}
            ordered_repos.append(view_data)

    # 4. æ¸²æŸ“ Markdown
    context = {
        "last_updated": store.data["last_updated"],
        "repos": ordered_repos,
    }

    output_md_path = SCRIPT_DIR / cfg["output"].get("file_path", "stars.md")
    md_content = generator.render(DEFAULT_MD_TEMPLATE, context)
    output_md_path.write_text(md_content, encoding="utf-8")
    log.info(f"âœ… Markdown ç”Ÿæˆå®Œæˆ: {output_md_path}")

    # 5. å¯é€‰ï¼šVault åŒæ­¥
    v_cfg = cfg.get("vault_sync", {})
    if v_cfg.get("enabled"):
        gh.push_file(
            v_cfg["repo"],
            v_cfg.get("file_path", "stars.md"),
            md_content,
            v_cfg.get("commit_message", "automated update"),
            v_cfg["pat"],
        )

    log.info("åŒæ­¥ä»»åŠ¡ç»“æŸ")


if __name__ == "__main__":
    main()
